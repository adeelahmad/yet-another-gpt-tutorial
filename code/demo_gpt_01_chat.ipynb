{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee5fb83",
   "metadata": {},
   "source": [
    "### How to chat with GPT using OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0533bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai version:[0.27.8]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from retrying import retry\n",
    "from IPython.display import Markdown,display\n",
    "print (\"openai version:[%s]\"%(openai.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900baa7d",
   "metadata": {},
   "source": [
    "### Locate where your key is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3b903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_path:[../key/rilab_key.txt]\n"
     ]
    }
   ],
   "source": [
    "key_path = '../key/rilab_key.txt'\n",
    "print ('key_path:[%s]'%(key_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49b03f",
   "metadata": {},
   "source": [
    "### Use key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2fc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(key_path, 'r') as f: OPENAI_API_KEY = f.read()\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd6db4",
   "metadata": {},
   "source": [
    "### Query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830439b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "@retry(stop_max_attempt_number=5,\n",
    "       wait_exponential_multiplier=1000,\n",
    "       wait_exponential_max=10000)\n",
    "def query_gpt(messages:list,gpt_model='gpt-4'):\n",
    "    \"\"\"\n",
    "        gpt_model: 'gpt-3.5-turbo' / 'gpt-4'\n",
    "    \"\"\"\n",
    "    # Call the OpenAI API\n",
    "    response     = openai.ChatCompletion.create(\n",
    "        model    = gpt_model, \n",
    "        messages = messages\n",
    "    )\n",
    "    # Extract the response content and status code\n",
    "    content     = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    status_code = response[\"choices\"][0][\"finish_reason\"]\n",
    "    return content,status_code,response\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48f062",
   "metadata": {},
   "source": [
    "`messages`, an input to GPT, is basically a list where each item is a dictionary consists of `role` and `content`. A `role` can either be\n",
    "* `system`: which defines the identity of the agent\n",
    "* `user`: which states the input of a user\n",
    "* `assistant`: which stores messages previously generated by the agents\n",
    "More information can be found in [here](https://platform.openai.com/docs/guides/gpt/chat-completions-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ab7761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '너는 한국어로 대화에 능통한 언어모델이야'}, {'role': 'user', 'content': 'LLM(Large Language Model)은 어떻게 작동하는지 설명해줘'}]\n"
     ]
    }
   ],
   "source": [
    "role_msg = \"\"\"너는 한국어로 대화에 능통한 언어모델이야\"\"\"\n",
    "question = \"LLM(Large Language Model)은 어떻게 작동하는지 설명해줘\"\n",
    "messages = [{\"role\": \"system\", \"content\": f'{role_msg}'},\n",
    "            {\"role\": 'user', \"content\": f'{question}'}]\n",
    "print (messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f4f53",
   "metadata": {},
   "source": [
    "### Now let's use `GPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ccc1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "content,status_code,response = query_gpt(messages=messages,gpt_model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74676205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM은 대규모 언어 모델(Large Language Model)의 약어로, 많은 양의 텍스트 데이터를 학습하여 자연어 이해와 생성 과정을 수행하는 모델을 지칭합니다. LLM은 다양한 자연어 처리 작업에 사용될 수 있으며, 이해, 생성, 번역, 요약, 질문 응답 등 다양한 언어 태스크를 수행할 수 있습니다.\n",
      "\n",
      "LLM은 주로 순환신경망(recurrent neural network, RNN)이나 변형된 변종인 장단기 메모리(long short-term memory, LSTM) 및 최근에는 변형 트랜스포머(transformer) 아키텍처를 기반으로 만들어집니다. 이러한 모델은 먼저 대량의 텍스트 데이터를 입력으로 사용하여 학습됩니다. 이 학습 단계에서 모델은 다음 단어를 예측하도록 훈련되며, 입력 텍스트의 맥락을 파악하고 다음 단어를 생성하는 방법을 학습합니다.\n",
      "\n",
      "학습 후, LLM은 이전에 본 적 없는 입력에 대해서도 유용한 출력을 생성할 수 있습니다. 이는 입력에 대한 최적의 출력을 찾기 위해 학습한 많은 수의 예시를 기반으로 유추하는 과정에서 가능해집니다. 따라서 LLM은 사용자의 입력에 대해 자연스러운 답변을 생성하거나, 지문을 요약하거나, 문장을 번역하는 등 다양한 언어 처리 작업을 수행할 수 있습니다.\n",
      "\n",
      "하지만 LLM은 학습 데이터에 내재된 편향이나 오류를 반영할 수 있으며, 모델이 악의적인 방식으로 활용될 가능성도 있습니다. 따라서 LLM을 사용할 때는 이러한 점을 고려하여 결과를 검증하고, 적절히 사용하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "print (content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d330b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n"
     ]
    }
   ],
   "source": [
    "print (status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9daf9aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7b0EwvvibDElr6tpZ3fqk7RbsPETA\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1689053414,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"LLM\\uc740 \\ub300\\uaddc\\ubaa8 \\uc5b8\\uc5b4 \\ubaa8\\ub378(Large Language Model)\\uc758 \\uc57d\\uc5b4\\ub85c, \\ub9ce\\uc740 \\uc591\\uc758 \\ud14d\\uc2a4\\ud2b8 \\ub370\\uc774\\ud130\\ub97c \\ud559\\uc2b5\\ud558\\uc5ec \\uc790\\uc5f0\\uc5b4 \\uc774\\ud574\\uc640 \\uc0dd\\uc131 \\uacfc\\uc815\\uc744 \\uc218\\ud589\\ud558\\ub294 \\ubaa8\\ub378\\uc744 \\uc9c0\\uce6d\\ud569\\ub2c8\\ub2e4. LLM\\uc740 \\ub2e4\\uc591\\ud55c \\uc790\\uc5f0\\uc5b4 \\ucc98\\ub9ac \\uc791\\uc5c5\\uc5d0 \\uc0ac\\uc6a9\\ub420 \\uc218 \\uc788\\uc73c\\uba70, \\uc774\\ud574, \\uc0dd\\uc131, \\ubc88\\uc5ed, \\uc694\\uc57d, \\uc9c8\\ubb38 \\uc751\\ub2f5 \\ub4f1 \\ub2e4\\uc591\\ud55c \\uc5b8\\uc5b4 \\ud0dc\\uc2a4\\ud06c\\ub97c \\uc218\\ud589\\ud560 \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\\n\\nLLM\\uc740 \\uc8fc\\ub85c \\uc21c\\ud658\\uc2e0\\uacbd\\ub9dd(recurrent neural network, RNN)\\uc774\\ub098 \\ubcc0\\ud615\\ub41c \\ubcc0\\uc885\\uc778 \\uc7a5\\ub2e8\\uae30 \\uba54\\ubaa8\\ub9ac(long short-term memory, LSTM) \\ubc0f \\ucd5c\\uadfc\\uc5d0\\ub294 \\ubcc0\\ud615 \\ud2b8\\ub79c\\uc2a4\\ud3ec\\uba38(transformer) \\uc544\\ud0a4\\ud14d\\ucc98\\ub97c \\uae30\\ubc18\\uc73c\\ub85c \\ub9cc\\ub4e4\\uc5b4\\uc9d1\\ub2c8\\ub2e4. \\uc774\\ub7ec\\ud55c \\ubaa8\\ub378\\uc740 \\uba3c\\uc800 \\ub300\\ub7c9\\uc758 \\ud14d\\uc2a4\\ud2b8 \\ub370\\uc774\\ud130\\ub97c \\uc785\\ub825\\uc73c\\ub85c \\uc0ac\\uc6a9\\ud558\\uc5ec \\ud559\\uc2b5\\ub429\\ub2c8\\ub2e4. \\uc774 \\ud559\\uc2b5 \\ub2e8\\uacc4\\uc5d0\\uc11c \\ubaa8\\ub378\\uc740 \\ub2e4\\uc74c \\ub2e8\\uc5b4\\ub97c \\uc608\\uce21\\ud558\\ub3c4\\ub85d \\ud6c8\\ub828\\ub418\\uba70, \\uc785\\ub825 \\ud14d\\uc2a4\\ud2b8\\uc758 \\ub9e5\\ub77d\\uc744 \\ud30c\\uc545\\ud558\\uace0 \\ub2e4\\uc74c \\ub2e8\\uc5b4\\ub97c \\uc0dd\\uc131\\ud558\\ub294 \\ubc29\\ubc95\\uc744 \\ud559\\uc2b5\\ud569\\ub2c8\\ub2e4.\\n\\n\\ud559\\uc2b5 \\ud6c4, LLM\\uc740 \\uc774\\uc804\\uc5d0 \\ubcf8 \\uc801 \\uc5c6\\ub294 \\uc785\\ub825\\uc5d0 \\ub300\\ud574\\uc11c\\ub3c4 \\uc720\\uc6a9\\ud55c \\ucd9c\\ub825\\uc744 \\uc0dd\\uc131\\ud560 \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4. \\uc774\\ub294 \\uc785\\ub825\\uc5d0 \\ub300\\ud55c \\ucd5c\\uc801\\uc758 \\ucd9c\\ub825\\uc744 \\ucc3e\\uae30 \\uc704\\ud574 \\ud559\\uc2b5\\ud55c \\ub9ce\\uc740 \\uc218\\uc758 \\uc608\\uc2dc\\ub97c \\uae30\\ubc18\\uc73c\\ub85c \\uc720\\ucd94\\ud558\\ub294 \\uacfc\\uc815\\uc5d0\\uc11c \\uac00\\ub2a5\\ud574\\uc9d1\\ub2c8\\ub2e4. \\ub530\\ub77c\\uc11c LLM\\uc740 \\uc0ac\\uc6a9\\uc790\\uc758 \\uc785\\ub825\\uc5d0 \\ub300\\ud574 \\uc790\\uc5f0\\uc2a4\\ub7ec\\uc6b4 \\ub2f5\\ubcc0\\uc744 \\uc0dd\\uc131\\ud558\\uac70\\ub098, \\uc9c0\\ubb38\\uc744 \\uc694\\uc57d\\ud558\\uac70\\ub098, \\ubb38\\uc7a5\\uc744 \\ubc88\\uc5ed\\ud558\\ub294 \\ub4f1 \\ub2e4\\uc591\\ud55c \\uc5b8\\uc5b4 \\ucc98\\ub9ac \\uc791\\uc5c5\\uc744 \\uc218\\ud589\\ud560 \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\\n\\n\\ud558\\uc9c0\\ub9cc LLM\\uc740 \\ud559\\uc2b5 \\ub370\\uc774\\ud130\\uc5d0 \\ub0b4\\uc7ac\\ub41c \\ud3b8\\ud5a5\\uc774\\ub098 \\uc624\\ub958\\ub97c \\ubc18\\uc601\\ud560 \\uc218 \\uc788\\uc73c\\uba70, \\ubaa8\\ub378\\uc774 \\uc545\\uc758\\uc801\\uc778 \\ubc29\\uc2dd\\uc73c\\ub85c \\ud65c\\uc6a9\\ub420 \\uac00\\ub2a5\\uc131\\ub3c4 \\uc788\\uc2b5\\ub2c8\\ub2e4. \\ub530\\ub77c\\uc11c LLM\\uc744 \\uc0ac\\uc6a9\\ud560 \\ub54c\\ub294 \\uc774\\ub7ec\\ud55c \\uc810\\uc744 \\uace0\\ub824\\ud558\\uc5ec \\uacb0\\uacfc\\ub97c \\uac80\\uc99d\\ud558\\uace0, \\uc801\\uc808\\ud788 \\uc0ac\\uc6a9\\ud558\\ub294 \\uac83\\uc774 \\uc911\\uc694\\ud569\\ub2c8\\ub2e4.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 60,\n",
      "    \"completion_tokens\": 572,\n",
      "    \"total_tokens\": 632\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cddce3",
   "metadata": {},
   "source": [
    "### Helper Class for implementing efficient chat with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46db1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "class GPTchatClass():\n",
    "    def __init__(self,\n",
    "                 gpt_model = 'gpt-4',\n",
    "                 role_msg  = 'Your are a helpful assistant.',\n",
    "                 VERBOSE   = True\n",
    "                ):\n",
    "        self.gpt_model     = gpt_model\n",
    "        self.messages      = [{'role':'system','content':f'{role_msg}'}]\n",
    "        self.init_messages = [{'role':'system','content':f'{role_msg}'}]\n",
    "        self.VERBOSE       = VERBOSE\n",
    "        self.response      = None\n",
    "        if self.VERBOSE:\n",
    "            print (\"Chat agent using [%s] initialized with the follow role:[%s]\"%\n",
    "                   (self.gpt_model,role_msg))\n",
    "    \n",
    "    def _add_message(self,role='assistant',content=''):\n",
    "        \"\"\"\n",
    "            role: 'assistant' / 'user'\n",
    "        \"\"\"\n",
    "        self.messages.append({'role':role, 'content':content})\n",
    "        \n",
    "    def _get_response_content(self):\n",
    "        if self.response:\n",
    "            return self.response['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def _get_response_status(self):\n",
    "        if self.response:\n",
    "            return self.response['choices'][0]['message']['finish_reason']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def chat(self,user_msg='hi',\n",
    "             PRINT_USER_MSG=True,PRINT_GPT_OUTPUT=True,\n",
    "             RESET_CHAT=False,RETURN_RESPONSE=True):\n",
    "        self._add_message(role='user',content=user_msg)\n",
    "        self.response = openai.ChatCompletion.create(\n",
    "            model    = self.gpt_model,\n",
    "            messages = self.messages\n",
    "        )\n",
    "        # Backup response for continous chatting\n",
    "        self._add_message(role='assistant',content=self._get_response_content())\n",
    "        if PRINT_USER_MSG:\n",
    "            print(\"[USER_MSG]\")\n",
    "            printmd(user_msg)\n",
    "        if PRINT_GPT_OUTPUT:\n",
    "            print(\"[GPT_OUTPUT]\")\n",
    "            printmd(self._get_response_content())\n",
    "        # Reset\n",
    "        if RESET_CHAT:\n",
    "            self.messages = self.init_messages\n",
    "        # Return\n",
    "        if RETURN_RESPONSE:\n",
    "            return self._get_response_content()\n",
    "print (\"Ready.\")              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240f58b",
   "metadata": {},
   "source": [
    "### Now let's chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c6ad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat agent using [gpt-3.5-turbo] initialized with the follow role:[Your are a helpful assistant.]\n",
      "[USER_MSG]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Who is the current president of Korea?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT_OUTPUT]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As of February 2022, the current president of South Korea is Moon Jae-in. However, please note that political situations can change, so it's always a good idea to stay updated with the latest news and official sources for the most accurate information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER_MSG]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Are you sure? I think you are outdated."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT_OUTPUT]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I apologize for any confusion. As an AI, I rely on the information available to me at the time of programming. Please verify with reliable sources for the most up-to-date information on the current president of South Korea."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER_MSG]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Where can I get the latest information?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT_OUTPUT]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To get the latest information about the current president of South Korea, you can refer to reliable news sources and official government websites. Here are some trusted sources you can consider:\n",
       "\n",
       "1. Official Government Websites: The official website of the South Korean government or the presidential office usually provides accurate and updated information on the current president.\n",
       "2. News Agencies: Reputable news agencies like BBC, CNN, Reuters, and Associated Press often publish news articles and updates on the current political situation in South Korea.\n",
       "3. Local South Korean News Outlets: News outlets based in South Korea, such as Korea Herald, Yonhap News Agency, and The Korea Times, can provide reliable and timely updates on the country's political affairs.\n",
       "\n",
       "By referring to these sources, you can gather the most accurate and up-to-date information regarding the current president of South Korea."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPT = GPTchatClass(gpt_model='gpt-3.5-turbo',role_msg  = 'Your are a helpful assistant.')\n",
    "PRINT_USER_MSG   = True\n",
    "PRINT_GPT_OUTPUT = True\n",
    "RESET_CHAT       = False\n",
    "RETURN_RESPONSE  = False\n",
    "GPT.chat(user_msg='Who is the current president of Korea?',\n",
    "         PRINT_USER_MSG=PRINT_USER_MSG,PRINT_GPT_OUTPUT=PRINT_GPT_OUTPUT,\n",
    "         RESET_CHAT=RESET_CHAT,RETURN_RESPONSE=RETURN_RESPONSE)\n",
    "GPT.chat(user_msg='Are you sure? I think you are outdated.',\n",
    "         PRINT_USER_MSG=PRINT_USER_MSG,PRINT_GPT_OUTPUT=PRINT_GPT_OUTPUT,\n",
    "         RESET_CHAT=RESET_CHAT,RETURN_RESPONSE=RETURN_RESPONSE)\n",
    "GPT.chat(user_msg='Where can I get the latest information?',\n",
    "         PRINT_USER_MSG=PRINT_USER_MSG,PRINT_GPT_OUTPUT=PRINT_GPT_OUTPUT,\n",
    "         RESET_CHAT=RESET_CHAT,RETURN_RESPONSE=RETURN_RESPONSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3dff58",
   "metadata": {},
   "source": [
    "### Chat with resetting everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b3982d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat agent using [gpt-3.5-turbo] initialized with the follow role:[Your are a helpful assistant.]\n",
      "[USER_MSG]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Who is the current president of Korea?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT_OUTPUT]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As of my last update in September 2021, the President of South Korea is Moon Jae-in. Note that political positions may change, so it's always good to verify with up-to-date sources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER_MSG]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Are you sure? I think you are outdated."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT_OUTPUT]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I apologize if I appear outdated to you. As an AI assistant, I am constantly learning and updating my knowledge base to provide accurate and helpful information. If there is anything specific you would like assistance with, please let me know and I'll do my best to help you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER_MSG]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Where can I get the latest information?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT_OUTPUT]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "There are several reliable sources where you can get the latest information:\n",
       "\n",
       "1. News websites and apps: Visit reputable news websites or download trusted news apps like BBC, CNN, Reuters, or Associated Press. They cover a wide range of topics and provide up-to-date news.\n",
       "\n",
       "2. Social media: Follow official accounts of news outlets, journalists, and experts on platforms like Twitter, Facebook, or LinkedIn. They often share breaking news and provide updates in real-time.\n",
       "\n",
       "3. Government websites: Official government websites and health organization websites are good sources for accurate and updated information on various topics, including health, policy changes, and current events.\n",
       "\n",
       "4. Mobile news notifications: Many news organizations offer mobile apps that send push notifications for breaking news and important updates. You can choose to receive notifications on specific topics or from specific sources.\n",
       "\n",
       "5. Podcasts: Subscribe to news and current affairs podcasts that offer timely discussions and analysis on various subjects. This can be a convenient way to stay informed while on-the-go.\n",
       "\n",
       "Remember to critically evaluate the sources and cross-check information whenever possible to ensure accuracy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPT = GPTchatClass(gpt_model='gpt-3.5-turbo',role_msg  = 'Your are a helpful assistant.')\n",
    "PRINT_USER_MSG   = True\n",
    "PRINT_GPT_OUTPUT = True\n",
    "RESET_CHAT       = True\n",
    "RETURN_RESPONSE  = False\n",
    "GPT.chat(user_msg='Who is the current president of Korea?',\n",
    "         PRINT_USER_MSG=PRINT_USER_MSG,PRINT_GPT_OUTPUT=PRINT_GPT_OUTPUT,\n",
    "         RESET_CHAT=RESET_CHAT,RETURN_RESPONSE=RETURN_RESPONSE)\n",
    "GPT.chat(user_msg='Are you sure? I think you are outdated.',\n",
    "         PRINT_USER_MSG=PRINT_USER_MSG,PRINT_GPT_OUTPUT=PRINT_GPT_OUTPUT,\n",
    "         RESET_CHAT=RESET_CHAT,RETURN_RESPONSE=RETURN_RESPONSE)\n",
    "GPT.chat(user_msg='Where can I get the latest information?',\n",
    "         PRINT_USER_MSG=PRINT_USER_MSG,PRINT_GPT_OUTPUT=PRINT_GPT_OUTPUT,\n",
    "         RESET_CHAT=RESET_CHAT,RETURN_RESPONSE=RETURN_RESPONSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b54979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
